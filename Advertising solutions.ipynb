{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries \n",
    "\n",
    "Before we import the data, let's load the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classification models\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree # for decision tree models\n",
    "from sklearn.ensemble import RandomForestClassifier # for Random Forest (ensemble) method\n",
    "from sklearn.svm import SVC  # import SVM classifier\n",
    "from sklearn.svm import LinearSVC # for a faster implementation of the linear SVM classifier - can also use SVC and kernel='linear'\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "\n",
    "This dataset represents a set of possible advertisements on Internet pages. The dataset can be obtained from \n",
    "https://archive.ics.uci.edu/ml/datasets/Internet+Advertisements, which is the UCI data\n",
    "repository. It can be addressed a supervised learning problem. The dataset is provided\n",
    "to you in the zip file as “ad.data”.\n",
    "The features encode:\n",
    "- the geometry of the image (if available)\n",
    "- phrases occuring in the URL\n",
    "- the image's URL and alt text\n",
    "- the anchor text,\n",
    "- words occuring near the anchor text\n",
    "\n",
    "**The task is to predict whether an image is an advertisement (\"ad\") or not (\"nonad\"). The aim is to classify based on the given features given the features mentioned**\n",
    "\n",
    "#### Importing the data \n",
    "\n",
    "As with the previous Labs, we will start by loading the provided dataset \"breast_cancer.csv\" into a `DataFrame` named **\"input_data\"** using once more the function  `pd.read_csv()` (Check the pandas [read_csv() documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) if needed). \n",
    "- To get acquainted with our data, let’s look at the first 5 entries using `head()`\n",
    "- Check and print the dimensionality of the data using `shape`\n",
    "- The dataset is provided in your Lab folder (no need to download it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3279, 1559)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1549</th>\n",
       "      <th>1550</th>\n",
       "      <th>1551</th>\n",
       "      <th>1552</th>\n",
       "      <th>1553</th>\n",
       "      <th>1554</th>\n",
       "      <th>1555</th>\n",
       "      <th>1556</th>\n",
       "      <th>1557</th>\n",
       "      <th>1558</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125</td>\n",
       "      <td>125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>468</td>\n",
       "      <td>8.2105</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>230</td>\n",
       "      <td>6.9696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>468</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ad.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>468</td>\n",
       "      <td>7.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ad.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1       2    3     4     5     6     7     8     9     ...  1549  \\\n",
       "0   125   125     1.0    1     0     0     0     0     0     0  ...     0   \n",
       "1    57   468  8.2105    1     0     0     0     0     0     0  ...     0   \n",
       "2    33   230  6.9696    1     0     0     0     0     0     0  ...     0   \n",
       "3    60   468     7.8    1     0     0     0     0     0     0  ...     0   \n",
       "4    60   468     7.8    1     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "   1550  1551  1552  1553  1554  1555  1556  1557  1558  \n",
       "0     0     0     0     0     0     0     0     0   ad.  \n",
       "1     0     0     0     0     0     0     0     0   ad.  \n",
       "2     0     0     0     0     0     0     0     0   ad.  \n",
       "3     0     0     0     0     0     0     0     0   ad.  \n",
       "4     0     0     0     0     0     0     0     0   ad.  \n",
       "\n",
       "[5 rows x 1559 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: header=None (!)\n",
    "input_data = pd.read_csv('ad.data', header=None)\n",
    "print(input_data.shape)\n",
    "\n",
    "input_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3279 entries, 0 to 3278\n",
      "Columns: 1559 entries, 0 to 1558\n",
      "dtypes: int64(1554), object(5)\n",
      "memory usage: 39.0+ MB\n"
     ]
    }
   ],
   "source": [
    "input_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ? in the raw data with the vale np.nan, which means not a number\n",
    "#this denotes the missing values\n",
    "input_data.replace(to_replace ='[ ]*\\?', value = np.nan, regex = True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4       False\n",
       "        ...  \n",
       "1554    False\n",
       "1555    False\n",
       "1556    False\n",
       "1557    False\n",
       "1558    False\n",
       "Length: 1559, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.538884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.477890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.752364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.457457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1559 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      percent_missing\n",
       "0           27.538884\n",
       "1           27.477890\n",
       "2           27.752364\n",
       "3            0.457457\n",
       "4            0.000000\n",
       "...               ...\n",
       "1554         0.000000\n",
       "1555         0.000000\n",
       "1556         0.000000\n",
       "1557         0.000000\n",
       "1558         0.000000\n",
       "\n",
       "[1559 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = pd.DataFrame({'percent_missing': input_data.isnull().sum() * 100 / len(input_data)})\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split the data into input variable X and class vector y\n",
    "\n",
    "Decision Trees and Random Forests follow a similar workflow to other supervised models in `sklearn`. We need to first start by setting the `X` matrix (input feature matrix) and `y` vector (class target):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_var = 1558\n",
    "\n",
    "X = input_data.drop(class_var, axis=1)\n",
    "y = input_data[class_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions:  (3279, 1558)\n",
      "y dimensions:  (3279,)\n"
     ]
    }
   ],
   "source": [
    "print (\"X dimensions: \", X.shape)\n",
    "print (\"y dimensions: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>...</th>\n",
       "      <th>1548</th>\n",
       "      <th>1549</th>\n",
       "      <th>1550</th>\n",
       "      <th>1551</th>\n",
       "      <th>1552</th>\n",
       "      <th>1553</th>\n",
       "      <th>1554</th>\n",
       "      <th>1555</th>\n",
       "      <th>1556</th>\n",
       "      <th>1557</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "      <td>3279.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.004270</td>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.004880</td>\n",
       "      <td>0.009149</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.012809</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.009759</td>\n",
       "      <td>0.001525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.065212</td>\n",
       "      <td>0.107042</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>0.062850</td>\n",
       "      <td>0.107042</td>\n",
       "      <td>0.057831</td>\n",
       "      <td>0.069694</td>\n",
       "      <td>0.095227</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077872</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>0.060393</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.055148</td>\n",
       "      <td>0.079783</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.115077</td>\n",
       "      <td>0.098320</td>\n",
       "      <td>0.039026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1554 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              4            5            6            7            8     \\\n",
       "count  3279.000000  3279.000000  3279.000000  3279.000000  3279.000000   \n",
       "mean      0.004270     0.011589     0.004575     0.003355     0.003965   \n",
       "std       0.065212     0.107042     0.067491     0.057831     0.062850   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              9            10           11           12           13    ...  \\\n",
       "count  3279.000000  3279.000000  3279.000000  3279.000000  3279.000000  ...   \n",
       "mean      0.011589     0.003355     0.004880     0.009149     0.004575  ...   \n",
       "std       0.107042     0.057831     0.069694     0.095227     0.067491  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  ...   \n",
       "\n",
       "              1548         1549         1550         1551         1552  \\\n",
       "count  3279.000000  3279.000000  3279.000000  3279.000000  3279.000000   \n",
       "mean      0.006099     0.004575     0.003660     0.002440     0.003050   \n",
       "std       0.077872     0.067491     0.060393     0.049341     0.055148   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "              1553         1554         1555         1556         1557  \n",
       "count  3279.000000  3279.000000  3279.000000  3279.000000  3279.000000  \n",
       "mean      0.006404     0.012809     0.013419     0.009759     0.001525  \n",
       "std       0.079783     0.112466     0.115077     0.098320     0.039026  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 1554 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the class frequencies\n",
    "\n",
    "An important aspect to understand before applying any classification algorithm is how the output labels are distributed. Are they evenly distributed or not? Imbalances in distribution of labels can often lead to poor classification results for the minority class even if the classification results for the majority class are very good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nonad.    2820\n",
       "ad.        459\n",
       "Name: 1558, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of adverisements and non-avertisements\n",
    "\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try to plot in a barplot using the sns.countplot() with 'data=input_data' and \n",
    "# 'x' equal to the feature that contains the target variable \n",
    "\n",
    "sns.countplot(x=class_var, data=input_data)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping (encoding) the categorical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the class variable to be in machine-readable form and ready to be used by ML models, it needs to be encoded in a numerical format. `LabelEncoder` from `sklearn` can be used to encode target labels with value between `0` and `n_classes-1`. \n",
    "\n",
    "**This transformer should be used to encode target values, i.e. y, and not the input X** (in which case, we can use One Hot Encoding or other ways of encoding). Read more about [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) and [Transforming the prediction variable(y)](https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical values into numbers using the LabelEncoder from sklearn\n",
    "\n",
    "# Instantiate a LabelEncoder() object and save it to a new variable \"le\"\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder \"le\" using fit_transform() on y (pass it as a parameter) \n",
    "# Assign back to \"y\". The fit_transform() function takes a categorical column \n",
    "# and converts/maps it to numerical values.\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2820\n",
       "0     459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check once more the distribution of the binary class \n",
    "\n",
    "pd.DataFrame(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Classification\n",
    "\n",
    "For every classification model built with scikit-learn, we will follow four main steps:\n",
    "\n",
    "1. Building the classification model (using either default, pre-defined or optimized parameters)\n",
    "2. Training (*fitting*) the model\n",
    "3. Testing (*predicting*) the model\n",
    "4. Performance evaluation using various metrics.\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "Training and testing a classification model on the same dataset is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data (poor generalisation). To use different datasets for training and testing, we need to split our dataset into two disjoint sets: train and test (Holdout method).\n",
    "\n",
    "Use `sklearn`’s `train_test_split()` function to randomly split the data into train and test sets (visit the [train_test_split documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) and the  [model cross-validation documentation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train and y_train:  (2295, 1558) (2295,)\n",
      "Shape of x_test and y_test:  (984, 1558) (984,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, \n",
    "                                                    stratify=y, random_state=0)\n",
    "\n",
    "print('Shape of x_train and y_train: ', X_train.shape, y_train.shape)\n",
    "print('Shape of x_test and y_test: ',   X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: it’s good practice to split the train and test sets before doing any feature engineering and/or scaling to avoid data leakage.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing: fill the NAs \n",
    "\n",
    "When inputting the missing values from the test set you should use always use the parameters estimated from the training set. Here is why: when training a machine learning model, you are using the training data to estimate the distribution of a population (where all the data you have come from). In this process, the correct use of the test set is to validate the generalization ability of your model when you get new unseen data. You should always use the values estimated with the training set because you are not supposed to see the test set until new data comes to you.\n",
    "\n",
    "\n",
    "Using the values (mean/std) from the test set to impute the test set will bias the results of your model. Which will result in an optimistic version of how the model should perform. \n",
    "\n",
    "Reminder: \n",
    "- If missing values are numerical, fill with mean / median. If data distribution is normal use mean otherwise (skewed) median.\n",
    "- If missing values are categorical then directly use mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Imputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "# Fit the Imputer ONLY on the training data (and transform - this could be done using two separate steps/functions)\n",
    "X_train = imp.fit_transform(X_train)\n",
    "X_test  = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1553    False\n",
       "1554    False\n",
       "1555    False\n",
       "1556    False\n",
       "1557    False\n",
       "Length: 1558, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling \n",
    "\n",
    "Decision Trees and Random Forests need little to no data pre-processing so we can skip the step of Scaling / Normalization for today's Lab, mainly to highlight the feature splits in the following `plot_tree` visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the StandardScaler() or MinMaxScaler() from sklearn and store into a variable named \"scaler\" \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data (and transform - this could be done using two separate steps/functions) \n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Use the fitted scaler to transform the test data \n",
    "X_test  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers \n",
    "\n",
    "### 1a) Default KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.959349593495935\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Instantiate the KNeighborsClassifier() classifier using the default parameters \n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Step 2 - Fit the knn model to the training set (use knn.fit())\n",
    "# Pass as arguments the train matrix X_train and the class vec y_train \n",
    "# No need to assign it into a new variable\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Step 3 - Predict the test data using the knn model (use knn.predict())\n",
    "# Pass as argument only the test matrix X_test\n",
    "# Save the prediction output into a new variable \"y_pred\"\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Step 4 - Print the final overall accuracy for the test set using metrics.accuracy_score()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred\n",
    "\n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[102  36]\n",
      " [  4 842]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion_matrix for the test set using metrics.confusion_matrix()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.84       138\n",
      "           1       0.96      1.00      0.98       846\n",
      "\n",
      "    accuracy                           0.96       984\n",
      "   macro avg       0.96      0.87      0.91       984\n",
      "weighted avg       0.96      0.96      0.96       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification_report for the test set using metrics.classification_report()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Tuned KNN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters using grid search:  {'metric': 'manhattan', 'n_neighbors': 6, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "# Create the dictionary of model hyperparameters\n",
    "n_neighbors = np.arange(1, 51, 5)  \n",
    "weights     = ['uniform', 'distance']\n",
    "metric      = ['minkowski','euclidean','manhattan']\n",
    "\n",
    "# Convert to dictionary\n",
    "knn_param_grid = dict(n_neighbors = n_neighbors, \n",
    "                      weights = weights, \n",
    "                      metric = metric\n",
    "                     )\n",
    "knn_param_grid\n",
    "\n",
    "# Grid search with 10-fold cross-validation using a dictionary of parameters\n",
    "gridCV = GridSearchCV(KNeighborsClassifier(), \n",
    "                      knn_param_grid, \n",
    "                      cv=10)\n",
    "gridCV.fit(X_train, y_train) \n",
    "\n",
    "# Report the optimal parameters using\n",
    "print('Best Parameters using grid search: ', gridCV.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9603658536585366\n"
     ]
    }
   ],
   "source": [
    "# Build the classifier using the optimal parameters detected by grid search\n",
    "\n",
    "knn_opt = gridCV.best_estimator_\n",
    "knn_opt.fit(X_train, y_train)\n",
    "y_pred = knn_opt.predict(X_test)\n",
    "\n",
    "# Report the final overall accuracy\n",
    "print('Test set accuracy: ',metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decision Tree Classifier \n",
    "\n",
    "Decision Tree classifiers construct classification models in the form of a tree structure. A decision tree progressively splits the training set into smaller subsets. Each node of the tree represents a subset of the data. Once a new sample is presented to the data, it is classified according to the test condition generated for each node of the tree.\n",
    "\n",
    "<!-- #### Decision Tree Classifier parameters\n",
    "- `criterion`: The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "- `splitter`: The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split.\n",
    "- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "- `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "- `max_features`: The number of features to consider when looking for the best split.\n",
    "- `max_leaf_nodes`: Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n",
    "- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf. -->\n",
    "\n",
    "#### Decision Tree Classifier with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9695121951219512\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=0)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[119  19]\n",
      " [ 11 835]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion_matrix for the test set using metrics.confusion_matrix()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89       138\n",
      "           1       0.98      0.99      0.98       846\n",
      "\n",
      "    accuracy                           0.97       984\n",
      "   macro avg       0.95      0.92      0.94       984\n",
      "weighted avg       0.97      0.97      0.97       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification_report for the test set using metrics.classification_report()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### you can tune the parameters of the DecisionTree here  \n",
    "### but RFs (as follows) can be more powerful instead \n",
    "### DTs are notoriously prone to over-fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "Random Forest is one of the most popular and most powerful machine learning algorithms. Random forest is a supervised learning algorithm that is used for classification and regression tasks. The \"forest\" is an **ensemble of decision trees** (each of which is based on a random subset of the data). The general idea of the bagging method is that a combination of learning models reduces the chance of overfitting. \n",
    "\n",
    "#### Random Forest Classifier with pre-defined parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Instantiate the RandomForestClassifier() classifier using the default parameters\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "#  Print the final overall accuracy for the test set using metrics.accuracy_score()\n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91       138\n",
      "           1       0.98      0.99      0.99       846\n",
      "\n",
      "    accuracy                           0.98       984\n",
      "   macro avg       0.97      0.93      0.95       984\n",
      "weighted avg       0.98      0.98      0.98       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification_report for the test set using metrics.classification_report()\n",
    "# Pass as parameters the actual values from y_test and the predicted values from y_pred\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF hyperparameter tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters using grid search: \n",
      " {'n_estimators': 170, 'max_features': 'sqrt', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# Try RandomSearchCV() or GridSearchCV() (significantly slower) with \n",
    "# 5-fold or 10-fold cross-validation (cv=5 or cv=10)\n",
    "# (more cv folds reduces the chances of overfitting but also increases the run time) \n",
    "# using a dictionary of parameters such as the ones defined as follows  \n",
    "\n",
    "# Create the dictionary of hyperparameters \n",
    "param_grid = {'n_estimators': np.arange(10, 200, 10),\n",
    "              'max_depth': [np.arange(1, 10, 2), None],\n",
    "              'max_features' : ['sqrt', 'log2', None], \n",
    "#               'min_samples_split': [1, 3, 5, 10], \n",
    "#               'min_samples_leaf': [1, 3, 10],\n",
    "#               'criterion': ['gini', 'entropy'], \n",
    "             }\n",
    "\n",
    "# Set up the RandomSearchCV and assign to a new variable named cv_rf\n",
    "# The most important arguments in RandomizedSearchCV are n_iter, \n",
    "# which controls the number of different combinations to try, \n",
    "# and cv which is the number of folds to use for cross validation \n",
    "cv_rf = RandomizedSearchCV(RandomForestClassifier(random_state=0), \n",
    "                           param_distributions=param_grid, \n",
    "                           n_iter = 30,\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1,\n",
    "                           random_state=0)\n",
    "\n",
    "# Fit the grid or random search model to X_train and y_train \n",
    "cv_rf.fit(X_train, y_train)\n",
    "\n",
    "# Report the optimal parameters using 'cv_rf.best_params_'\n",
    "print('Best Parameters using grid search: \\n', cv_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features='sqrt', n_estimators=170, random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best model (with the optimal parameters) using 'cv_rf.best_estimator_'\n",
    "\n",
    "cv_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the final optimized model using the best parameters as detected from the exhaustive grid search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9745934959349594\n"
     ]
    }
   ],
   "source": [
    "# Build the classifier using the optimal parameters detected by the tuning process\n",
    "\n",
    "# Save the result cv_rf.best_estimator_ into a new variable rf_opt \n",
    "\n",
    "rf_opt = cv_rf.best_estimator_\n",
    "\n",
    "# Fit the optimal model rf_opt to the training set. Pass as arguments X_train and y_train\n",
    "rf_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data X_test. Use rf_opt.predict(). \n",
    "# Assign the result into a new variable y_pred \n",
    "y_pred = rf_opt.predict(X_test)\n",
    "\n",
    "# Report the final overall accuracy using metrics.accuracy_score(). \n",
    "# Pass as parameters y_test and y_pred for the test accuracy \n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       138\n",
      "           1       0.98      0.99      0.99       846\n",
      "\n",
      "    accuracy                           0.97       984\n",
      "   macro avg       0.96      0.93      0.95       984\n",
      "weighted avg       0.97      0.97      0.97       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking performance our model with metrics.classification report() \n",
    "# Pass as parameters y_test and y_pred \n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "#### RBF SVM with default parameters\n",
    "\n",
    "Default hyperparameter are `C=1.0`, `kernel=rbf` and `gamma=auto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9603658536585366\n"
     ]
    }
   ],
   "source": [
    "svc = SVC() \n",
    "svc.fit(X_train,y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with default parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9603658536585366\n"
     ]
    }
   ],
   "source": [
    "linear_svc = SVC(kernel='linear') \n",
    "linear_svc.fit(X_train, y_train)\n",
    "y_pred = linear_svc.predict(X_test)\n",
    "\n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters using grid search:  {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV() with 5-fold or 10-fold cross-validation (cv=5 or cv=10)\n",
    "# (more cv folds reduces the chances of overfitting but also increases the run time)   \n",
    "\n",
    "# Create the dictionary of hyperparameters \n",
    "param_grid = [ {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "                'kernel':['linear']},\n",
    "               {'C':[0.001, 0.01, 0.1, 1, 10, 100, 1000], \n",
    "                'kernel':['rbf'], \n",
    "                'gamma':[1, 0.1, 0.01, 0.001, 0.0001,'auto']},\n",
    "             ]\n",
    "\n",
    "# Set up the GridSearchCV and assign to a new variable named cv_svm\n",
    "grid_search = GridSearchCV(SVC(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "# Fit the grid or random search model to X_train and y_train \n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Report the optimal parameters using 'cv_svm.best_params_'\n",
    "print('Best Parameters using grid search: ', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=0.0001)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best model (with the optimal parameters) \n",
    "# using 'cv_svm.best_estimator_'\n",
    "\n",
    "grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy:  0.9664634146341463\n"
     ]
    }
   ],
   "source": [
    "# Build the classifier using the optimal parameters detected by the tuning process\n",
    "\n",
    "# Save the result cv_svm.best_estimator_ into a new variable svm_opt \n",
    "svm_opt = grid_search.best_estimator_\n",
    "\n",
    "# Fit the optimal model svm_opt to the training set. \n",
    "svm_opt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test data X_test. Use svm_opt.predict() \n",
    "y_pred = svm_opt.predict(X_test)\n",
    "\n",
    "# Report the final overall accuracy using metrics.accuracy_score(). \n",
    "print('Test set accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87       138\n",
      "           1       0.96      1.00      0.98       846\n",
      "\n",
      "    accuracy                           0.97       984\n",
      "   macro avg       0.97      0.89      0.92       984\n",
      "weighted avg       0.97      0.97      0.96       984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking performance our model with metrics.classification report() \n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
